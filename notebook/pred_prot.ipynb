{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GTデータの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate(data, dt=1):\n",
    "    \"\"\"\n",
    "    時系列データを線形補間し、ダウンサンプリングする。\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): 2次元配列で、最初の列が時刻、残りの列が対応するデータ値を表す。\n",
    "        dt (float, optional): ダウンサンプリングのための時間間隔。デフォルトは1。\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 指定された時間間隔で補間されたダウンサンプリングデータを含む2次元配列。各行はその時間間隔での補間値を表す。\n",
    "    \"\"\"\n",
    "\n",
    "    new_data = []\n",
    "    pick_time = dt\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        if data[i, 0] > pick_time:\n",
    "            x = [data[i - 1, 0], data[i, 0]]\n",
    "            y = [data[i - 1, :], data[i, :]]\n",
    "\n",
    "            a, b = np.polyfit(x, y, 1)\n",
    "            interpolated_value = a * pick_time + b\n",
    "\n",
    "            new_data.append(interpolated_value)\n",
    "            pick_time += dt\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_data = new_data.reshape(new_data.shape[0], -1)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_data(dataset_list, case_num, num_kw):\n",
    "    vis_dataset = dataset_list[case_num - 1]\n",
    "\n",
    "    gt = decimate(pd.read_csv(vis_dataset, skiprows=1).values)\n",
    "    gt = gt[:, -num_kw:]\n",
    "\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測値データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_data(gt, model_list, predict_dir, case_num, num_kw):\n",
    "\n",
    "    df_list = []\n",
    "    df_list.append(gt)\n",
    "\n",
    "    for model in model_list:\n",
    "        predict_dataset_path = os.path.join(predict_dir, model, f\"case{str(case_num).zfill(4)}.csv\")\n",
    "\n",
    "        # csvファイルから読み込んでkWの次元だけを抽出\n",
    "        pred_data = pd.read_csv(predict_dataset_path).values\n",
    "        pred_data = pred_data[:, -num_kw:]\n",
    "\n",
    "        # in_len部分をgtから拝借して結合\n",
    "        in_len = gt.shape[0] - pred_data.shape[0]\n",
    "        pred_data = np.concatenate((gt[:in_len], pred_data), axis=0)\n",
    "        df_list.append(pred_data)\n",
    "\n",
    "    df_list = np.stack(df_list, axis=0)\n",
    "\n",
    "    return df_list, in_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロットする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure(df_list, case_num, dataset_name, pred_name, pred_unit):\n",
    "    nrows = 2\n",
    "    ncols = 2\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(10*ncols, 7*nrows))\n",
    "\n",
    "    vis_list = [\"Ground Truth\", \"BaseTransformer\", \"LSTM\", \"DeepONet\", \"DeepOTransformer\", \"DeepOLSTM\"]\n",
    "\n",
    "    sns.set_palette(\"bright\", len(vis_list))\n",
    "    sns.set_context(\"talk\") # paper, notebook, talk, poster\n",
    "\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            sns.lineplot(data=pd.DataFrame(df_list[:, :, (nrows-1) * i + (ncols) * j].T, columns=vis_list), ax=axes[i][j])\n",
    "            sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "\n",
    "            axes[i][j].set_title(f\"{pred_name[(nrows-1) * i + (ncols) * j]} [{pred_unit[(nrows-1) * i + (ncols-1) * j]}]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    # plt.show()\n",
    "    os.makedirs(f\"/home/dockeruser/code/notebook/{dataset_name}\", exist_ok=True)\n",
    "    plt.savefig(f\"/home/dockeruser/code/notebook/{dataset_name}/case{str(case_num).zfill(4)}.png\", format=\"png\", dpi=300)\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パラメータ類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"data_step2\"\n",
    "\n",
    "csv_dir = \"/home/dockeruser/code/predict_csvs\"\n",
    "gt_dir = f\"/home/dockeruser/dataset/{dataset_name}\"\n",
    "predict_dir = \"/home/dockeruser/code/predict_csvs\"\n",
    "\n",
    "model_list = [\"bt\", \"lstm\", \"don\", \"dot\", \"dol\"]\n",
    "\n",
    "if dataset_name == \"data_step2\":\n",
    "    num_kw = 4\n",
    "elif dataset_name == \"data_refrig_only\":\n",
    "    num_kw = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = sorted(glob.glob(os.path.join(gt_dir, \"*.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_list = sorted(glob.glob(os.path.join(predict_dir, \"bt\", \"*.csv\")))\n",
    "case_num_list = []\n",
    "for case_path in case_list:\n",
    "    file_name = os.path.basename(case_path)  # ファイル名を取得\n",
    "    number_part = file_name.split(\"case\")[1].split(\".\")[0]  # \"case\"と\".csv\"で分割し、真ん中の部分を抽出\n",
    "\n",
    "    # 数字をint型に変換\n",
    "    number = int(number_part)\n",
    "    case_num_list.append(number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = [col.split(\".\")[0] for col in pd.read_csv(dataset_list[0], skiprows=0, dtype=str).columns]\n",
    "feature_unit = [col.split(\".\")[0] for col in pd.read_csv(dataset_list[0], skiprows=1, dtype=str).columns]\n",
    "\n",
    "pred_name = feature_name[-num_kw:]\n",
    "pred_unit = feature_unit[-num_kw:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まとめて実行\n",
    "for case_num in case_num_list:\n",
    "    gt = get_gt_data(dataset_list, case_num, num_kw)\n",
    "    df_list, in_len = get_pred_data(gt, model_list, predict_dir, case_num, num_kw)\n",
    "    create_figure(df_list, case_num, dataset_name, pred_name, pred_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一つずつ実行\n",
    "case_num = 56 # 64, 415, 173, 56\n",
    "\n",
    "gt = get_gt_data(dataset_list, case_num, num_kw)\n",
    "df_list, in_len = get_pred_data(gt, model_list, predict_dir, case_num, num_kw)\n",
    "create_figure(df_list, case_num, dataset_name, pred_name, pred_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
